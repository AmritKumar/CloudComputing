\documentclass{acm_proc_article-sp}
\makeatletter
\newif\if@restonecol
\makeatother
\let\algorithm\relax
\let\endalgorithm\relax
\usepackage{hyperref}
\usepackage[all]{hypcap}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{listings}
\usepackage{algorithm2e}
\usepackage{subfigure}
%\usepackage{amsmath}
%\usepackage{amssymb}
\hypersetup{
  bookmarks=true,         % show bookmarks bar?
  unicode=false,          % non-Latin characters in Acrobat’s bookmarks
  pdftoolbar=true,        % show Acrobat’s toolbar?
  pdfmenubar=true,        % show Acrobat’s menu?
  pdffitwindow=false,     % window fit to page when opened
  pdfstartview={FitH},    % fits the width of the page to the window
  pdftitle={My title},    % title
  pdfauthor={Author},     % author
  pdfsubject={Subject},   % subject of the document
  pdfcreator={Creator},   % creator of the document
  pdfproducer={Producer}, % producer of the document
  pdfkeywords={keyword1} {key2} {key3}, % list of keywords
  pdfnewwindow=true,      % links in new window
  colorlinks=true,       % false: boxed links; true: colored links
  linkcolor=cyan,          % color of internal links (change box color with linkbordercolor)
  citecolor=green,        % color of links to bibliography
  filecolor=magenta,      % color of file links
  urlcolor=cyan           % color of external links
}
\usepackage{listings}
\usepackage{algorithm2e}
\usepackage{verbatim}
\begin{document}
\title{Analyzing Security-Cost Trade-off for a \\ Fully Homomorphic Encryption Scheme}


\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
\alignauthor
Kais Chaabouni \\
       \affaddr{ENSIMAG}\\
       \affaddr{Grenoble INP}\\
       \affaddr{Grenoble, France}\\
       \email{kais.chaabouni@ensimag.imag.fr}
% 2nd. author
\alignauthor 
Amrit Kumar\\
       \affaddr{ENSIMAG-Ecole Polytechnique}\\
       \affaddr{Grenoble INP}\\
       \affaddr{Grenoble, France}\\
       \email{amrit.kumar@ensimag.imag.fr}
}

% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.

\date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.
\maketitle
\begin{abstract}
This paper investigates the feasibility of transformation of a (possibly) non-straight-line program (on unencrypted data) into a straight-line program on encrypted data. We present an analysis of security-cost trade-off for a homomorphic encryption scheme executing such programs. Analysis is based on the measurements (CPU time and  Wall time) taken for programs : performing \texttt{XOR} of an $n$-bit sequence,  evaluating the sum of two $n$-bit integers, determining the majority bit of an $n$-bit sequence, finding the product of two $n \times n$ matrices, and sorting $n$ bit sequences of length $nbits$ (usually requiring validity of a boolean predicate). The evaluation is performed on an available implementation ``Scarab library'' of a fully homomorphic encryption scheme. 

\end{abstract}

\keywords{ Fully Homomorphic Encryption, Security, Cost.} % NOT required for Proceedings

\section{Introduction}

The notion of \textit{fully homomorphic encryption scheme}, originally called a \textit{privacy homomorphism} was introduced by Rivest et al. \cite{rivest78} shortly after the invention of RSA by Rivest, Shamir and Adleman \cite{Rivest78amethod}.  Basic RSA is a multiplicative homomorphic encryption scheme -- i.e. given the public parameters $pk:=(e, N)$ and two messages $m_0, m_1$, their encryption $c_0, c_1$ verify $c_0c_1=(m_0m_1)^e \ \textrm {mod}\ N$. Hence, without knowing the associated secret key, an external agent can compute the encryption of the product of the messages.

Fully homomorphic encryption scheme extends the above property to addition. A fully homomorphic encryption scheme is a tuple of an encryption $\mathcal{E}$, decryption $\mathcal{D}$ algorithm together with an efficient algorithm $Eval_\mathcal{E}$ that evaluates any circuit $C$ on any cipher-texts $c_i \leftarrow \mathcal{E}(pk, m_i)$. The property that it can evaluate the encryption of addition and multiplication of two plain-texts (without access to the secret key) allows it to arbitrarily compute any boolean function on encrypted data without the decryption key. However, we assume that no branching is involved. Prior to \cite{homenc}, we did not have a viable construction.

As an application, such schemes can be used to query encrypted data stored on a remote machine. A user prepares his encrypted input data $c_0, c_1, \ldots, c_{n-1}$  and a description of the query i.e. a circuit $C$ he wants to evaluate and sends them to the remote machine. The machine transforms the circuit into $C^{'}$ so that it can evaluate the same query but on encrypted data, performs the operation and returns the encrypted result to the user. The user eventually decrypts the result. 

The immediate question that is raised, is how do we perform the transformation and how does the circuit size change to adapt to computations on encrypted data, especially when the function involves \texttt{if-then-else} condition. We provide an answer by analyzing circuit complexity for finding the minimum-maximum of two $n$-bit sequences in \autoref{sec:ex} and for some other operations in \autoref{sec:bm}. Moreover, the cost of using a fully homomorphic encryption scheme increases with the depth of the circuit representing the $Eval_{\mathcal{E}}$ function. 

Some researchers have improved Gentry's scheme and proposed other schemes such as \cite{cryptoeprint:2009:571} and \cite{cryptoeprint:2011:277} which improve the complexity of the scheme. In \autoref{Sec:Eval} we experimentally evaluate an implementation of  \cite{cryptoeprint:2009:571} and analyze the security-cost trade-off for $Eval_{\mathcal{E}}$ varying from \texttt{XOR} of an $n$-bit sequence to sum of two $n$-bit sequences to sorting of $n$ sequences of $nbits$ each. Three different sorting algorithms are tested.  
Through these experiments, we also provide information on the additional cost to be paid (for these computations) when a higher security is desired.

\section{Overview of Fully homomorphic Encryption scheme (FHE) }

The homomorphic encryption scheme proposed by Gentry \cite{homenc} is based on ideal lattices. Gentry's non-deterministic scheme builds on a \textit{somewhat homomorphic encryption}(SWHE) scheme which can perform additions and multiplications on encrypted data till a bounded depth. The non-determinism introduced in the form of noise renders the scheme secure. However, the noise increases with the depth  of the evaluation circuit and hence the decryption algorithm fails to produce the correct plain-text after a certain depth. To counter this problem, Gentry introduces \textit{bootstrapping} technique which ensures unbounded depth operatability of the scheme. Smart and Vercauteren \cite{cryptoeprint:2009:571} have proposed an integer based approach and we use one of its implementations detailed in \cite{perl:poster} for our experiments.

\subsection{Smart-Vercauteren  scheme}

The scheme proposed in \cite{cryptoeprint:2009:571} has 3 parameters : $(N=2^n, \eta, \mu)$, where $n\in \mathbb{N}$. The somewhat homomorphic encryption scheme uses 5 algorithms : (\texttt{KeyGen}, \texttt{Encrypt}, \texttt{Decrypt}, \texttt{Add}, \texttt{Mult}). Algorithm \autoref{Code:SV} presents the \texttt{KeyGen} algorithm. This scheme will be referred to as SV scheme in this paper.

\restylealgo{algoruled}
\linesnumbered
\begin{algorithm}[H]
 \SetVline
 \KwData{$N, \eta, \mu$ }
\KwResult{($pk, sk$)}
 $F(x)$ monic irreducible over $\mathbb{Z}[x]$ of degree $N$\;
\While{$p$ is not prime}{
  $S(x) \leftarrow_{R}B_{\infty , N}(\eta/2)$\; \tcc{Random polynomial in the ball}
  $G(x)=1+2.S(x)$\;
  $p= \mathrm{\textbf{resultant}}(G(x),F(x))$\;						
}

$D(x)=\mathrm{\textbf{gcd}}(G(x),F(x))$ over $\mathbb{F}_p[x]$\;
$\alpha$ $\leftarrow$ unique root of $D(x) \in \mathbb{F}_p $ \;
apply \texttt{xgcd} algorithm over $\mathbb{Q}[x]$ to obtain $Z(x) = \sum_{i=0}^{n-1}{z_ix^i} \in \mathbb{Z}[x] $  such that $Z(x).G(x)=p$ mod $F(x)$\;
$B=z_0$ mod $2p$\;
$pk = (p, \alpha)$ and $sk = (p , B)$\;
return ($pk$, $sk$)\;
\caption{KeyGen\label{Code:SV}}
\end{algorithm}

where $ B_{\infty , k}(r) = \{\sum_{i=0}^{k-1}{a_i}x^i : -r \leq a_i \leq r \} $ \newline \newline
\texttt{Encrypt}($m \in \{0,1\} , pk$): 
\newline \phantom{x}\hspace{3ex}  $R(x) \leftarrow_{R}B_{\infty , N}(\mu/2)$
 \; $C(x)=m+2\cdot R(x)$ 
\newline \phantom{x}\hspace{3ex} return  $c=C(\alpha)$ mod $p$;
\newline \texttt{Decrypt}($c, sk$):
\newline \phantom{x}\hspace{3ex} $m = (c - \lfloor c \cdot B/p \rceil )$ mod $2$;
\phantom{x}\hspace{1ex} return $m$;
\newline \texttt{Add}($c_1$, $c_2$, $pk$):
\newline \phantom{x}\hspace{3ex} $c_3=c_1+c_2$ mod $p$; 
\phantom{x}\hspace{1ex}  return $c_3$;
\newline \texttt{Mult}($c_1$, $c_2$, $pk$):
 \newline \phantom{x}\hspace{3ex}  $c_3=c_1.c_2$ mod $p$; 
\phantom{x}\hspace{1ex}  return $c_3$; 

\textbf{Security of SWHE scheme :} The scheme is semantically secure under the assumption that the \textit{Polynomial Coset Problem}(PCP) is hard. One-wayness of the encryption is believed to be true under the assumption that the \textit{Bounded distance decoding problem} BDDP is hard while key recovery is assumed to be difficult under the assumption that the \textit{Small Principal Ideal Problem}(SPIP) is hard. Solutions to all of these problems have either a sub-exponential algorithm or in general take exponential time.

If we choose $F(x)$ \footnote{a choice that authors of SV scheme propose and is always a safe choice for monic and irreducible polynomial over $\mathbb{Z}[x] $} to be $x^N+1$, and take \begin{equation}2 ^ \epsilon = \frac{\eta}{2\cdot \mu \sqrt{N}} \end{equation} then the SV scheme provides $N /\epsilon$ bits of security.

\textbf{SWHE to FHE :} We recall that the authors of the scheme have proved that the multiplicative depth $d$ of SWHE scheme verifies: \begin{equation}d \cdot \mathrm{log}(2) \leq  \mathrm{log}\left( \mathrm{log}\left(\frac{\eta}{2\cdot \sqrt{N}}\right) \right) - \mathrm{log}\left(\mathrm{log}\left( N\mu\right)\right)\end{equation}
when $F(x)= x^N+1$. Turning the SWHE scheme to FHE scheme is achieved by recasting Gentry's method for the parameters of this scheme. To achieve this, authors define another algorithm \texttt{Recrypt} which takes a cipher-text $c$ and re-encrypts it to $c_{new}$, whilst at the same time removing some of the noise in $c$. This requires augmenting the encryption key with some additional information by extending the \texttt{KeyGen} algorithm. The FHE scheme now generates $s_1$ uniformly random integers $B_i$ in $[-p, \ldots, p]$ such that there exists a subset $S$ of $s_2$ elements with $\sum_{j \in S}{B_j}= B$ over integers. By defining $sk_i =1$ iff $i\in S$(else $sk_i=0$) and taking $c_i=\texttt{Encrypt}(sk_i,pk)$ we generate a new public key which contains hint of the secret key $sk$. The public key now comprises of \[(p, \alpha, s_1, s_2, \{c_i, B_i\}_{i=1}^{s1})\]
To ensure that the hint does not provide any considerable advantage to an attacker, the underlying \textit{sparse subset-sum problem} SSSP must remain hard. If we take $s_1$ to be slightly greater than $\mathrm{log}_{2}(p)$, then we need to select $s_2$ such that $\sqrt {{s_1}  \choose {s_2} } > 2 ^{N/\epsilon}$ so as to ensure that SSSP difficulty is at least as difficult as the difficulty of the BDDP underlying the SWHE scheme. 

\subsection{Considered Implementations}
We used the open source implementation Scarab library \cite{hcrypt}  version $1.0.0$ of the SV scheme which is a part of \textsc{hcryptProject}. The implementation is in beta version. The software requires GMP(we tested with version $5.1.2$) \cite{gmp} : GNU Multiple Precision Arithmetic Library; FLINT version $1.6$ \cite{flint}: Fast Library for Number Theory; MPIR (we tested with $2.6.0$) \cite{mpir} : Multiple-Precision Integers and Rationals and MPFR (tested with version $3.1.2$) \cite{mpfr} :  Multiple-Precision Floating-point computations with correct Rounding. Linux is supposed to be the only supported platform. The libraries are implemented in  C and an installation guide is provided with the package. 

\autoref{funs} presents the list of functions provided by Scarab with their description. The library provides a file \texttt{test.c} where a user can write his own functions to test different operations. It also provides a header file \texttt{parameters.h} to set the parameters $N, \eta, \mu, s_1$ and $ s_2$ of the cryptosystem. To simplify our computational circuit, we added two basic functions \texttt{or(mpz\_t res, mpz\_t a, mpz\_t b, fhe\_pk\_t pk);} and \texttt{not(mpz\_t res, mpz\_t a, fhe\_pk\_t pk);} which compute the logical \texttt{or}(denoted by $\vee$) of encrypted bits and the negation (denoted by $\neg$) of an encrypted bit respectively. In this article we denote logical \texttt{AND} using $\wedge$ and \texttt{XOR} using $\oplus$.

\begin{table*}
\centering
\caption{Functions available in Scarab}
\begin{tabular}{|l|l||}
  \hline
  \textbf{function prototype} & \textbf{semantics}  \\
  \hline
 \texttt{fhe\_keygen(fhe\_pk\_t pk, fhe\_sk\_t sk);}  & 	Generate a keypair \\
\texttt{fhe\_encrypt(mpz\_t c, fhe\_pk\_t pk, int m);} &	Encrypt a message (0 or 1) \\
\texttt{fhe\_decrypt(mpz\_t c, fhe\_sk\_t sk);} &	Decrypt a cyphertext\\
\texttt{fhe\_recrypt(mpz\_t c, fhe\_pk\_t pk, fhe\_sk\_t sk); }	&Recrypt a cyphertext ( ``refreshing'' it) \\
\texttt{fhe\_add(mpz\_t res, mpz\_t a, mpz\_t b, fhe\_pk\_t pk);} &	Add cypher-texts (= XOR) \\
\texttt{fhe\_mul(mpz\_t res, mpz\_t a, mpz\_t b, fhe\_pk\_t pk);} &	Multiply cypher-texts (= AND) \\
\texttt{fhe\_fulladd(mpz\_t sum, mpz\_t c\_out, mpz\_t a, mpz\_t b,} &	Add with carry in and carry out \\
\phantom{x}\hspace{12ex} \texttt{ mpz\_t c\_in, fhe\_pk\_t pk);} & \\
\texttt{fhe\_halfadd(mpz\_t sum, mpz\_t c\_out, mpz\_t a,} &	Add with carry out \\
\phantom{x}\hspace{12ex} \texttt{ mpz\_t b, fhe\_pk\_t pk);} & \\  
\hline
\end{tabular}
\label{funs}
\end{table*}

We time the various algorithms on two desktop machines. Both these machines were x86-64 platform running Linux kernel $3.2.0$-$45$-generic  and used GCC $4.6.3$ C compiler.  The first machine housed Intel Core i3 M330 (2.13GHz x 4) processor and will be referred in this article as machine M1 while the other housed Intel Core i5 M2430 (2.40GHz x 4) processor and will be referred as M2. During the experiments these machines were not connected to any network and were running only the most needed applications.
 
\subsection{Cost of the Cryptosystem}
 In the SV scheme, performing any operation on encrypted bits requires  \texttt{KeyGen}, \texttt{Encrypt}, \texttt{Add},\texttt{ Mult}, \texttt{Recrypt}, \texttt{Decrypt}, and hence we measure their performance in terms of run time. These measurements are done by varying the parameter $N$(keeping all other parameters fixed) that determines the degree of all intermediate polynomials of the cryptosystem. The measurements in this section were taken on M2. From \autoref{common cost}, we can infer that the cost of these functions increase with the increase of the degree of the polynomial $F(x)$. Futhermore, we observe that the size of the public key is increasing with $N$ which can be explained by the fact that the resultant of 2 polynomials increases with the degree which makes the prime number $p$ difficult to be found and hence it increases the duration of key generation. The reasoning extends to \texttt{Encrypt} which evaluates a polynomial of higher degree. As for the rest of functions they depend on the size of ciphertext which becomes bigger with $p$.

\begin{table}[htb]
  \centering
  \caption{Run time (ms)}
\resizebox{9 cm}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c||}
  \hline
  \multicolumn{8}{|c||}{$\mu = 4$ , $\eta = 2^{384}$} \\
  \hline
  $N=2^{n}$ & $log_{2}(p)$ & KeyGen & Encrypt & Add & Mult & Recrypt & Decrypt \\
  \hline
  $2^{1}$ & 764 & 99 & 0.057 & 0.0 & 2.372 & 2.371 & 0.0 \\
  $2^{2}$ & 1535 & 221 & 0.072 & 0.0 & 	3.012 & 3.023 & 0.001\\
  $2^{3}$ & 3077 & 1131 & 0.079 & 0.001 & 3.396 & 3.380 & 0.004 \\
  $2^{4}$ & 6155 & 8560 & 0.135 & 0.002 & 5.881 & 5.889 & 0.012\\
  $2^{5}$ & 12329 & 852849 & 0.477 & 0.005 & 20.230 & 20.057 & 0.028\\
  \hline
\end{tabular}
\normalsize
\label{common cost}
}
\end{table}

\autoref{fig:image_f2} measures execution time for \texttt{KeyGen} for fixed values of parameters: $N=8$ and $\eta = 2^{384}$. It shows a variation from 0.2 s to 14.41 s with a very high density in the interval [0.2s, 2s].

\begin{figure}[!h] %on ouvre l'environnement figure
\centering
\includegraphics[width=5cm, height=4.5cm]{f3.pdf} 
\caption{Run time of KeyGen(ms)} 
\label{fig:image_f2} %l'étiquette pour faire référence à cette image
\end{figure}

As the time taken by \texttt{KeyGen} depends on how fast the prime $p$ is found, its cost is related with the density of primes i.e. $\frac{1}{\mathrm{log(n)}}$. \autoref{fig:image_f2} is in accordance with the density of primes over $\mathbb{N}$.

\subsection{Security }

Using (1), and (2), we calculate the theoretical security level and the depth provided by the cryptosystem, and we find that the security level increases with increase in $N$ and for $\mu =4$ and $\eta=2^{384}$ fixed. \autoref{common cost} asserts that  \texttt{KeyGen} takes 15mins for $N=2^5$. We note that for $N \geq 2^6$, \texttt{KeyGen} takes hours. Hence, we choose $N=8$ for our experiments which provides $1.01472$ level of security and we vary $\mu$ and $\eta$. Due to some erroneous results provided by the decryption function for $d<5.3$, we choose $\mu$ and $\eta$ to ensure that the depth is greater than the threshold value $5.9$.

\begin{table}[htb]
  \centering
  \caption{Security, Depth}
\begin{tabular}{|c|c|c||}
  \hline
  \multicolumn{3}{|c||}{$\mu = 4$ , $\eta = 2^{384}$} \\
  \hline
  $N=2^{n}$ &  $2^{\frac{N}{\epsilon}}$ & d \\
  \hline
  $2^{1}$ &  1.00365 & 6.99 \\
  $2^{2}$ &  1.00732 & 6.57\\
  $2^{3}$ &  1.01472 & 6.25\\
  $2^{4}$ &  1.02969 & 5.98\\
  $2^{5}$ & 1.06035 & 5.76\\
  \hline
\end{tabular}
\normalsize
\label{tab:securitytab}
\end{table}

\autoref{tab:tradeofftab1} presents the security-cost trade-off for 10000 multiplications as a function of $\eta$. We remark that : decreasing $\eta$ with other parameters fixed increases the security of the system at the cost of decreasing the depth. This confirms equation (1) and (2).  We further note that increasing the value of $\eta$ increases the computational cost as $p$ increases which makes the the ciphertexts longer. On the contrary, \autoref{tab:tradeofftab2} presents the security-cost tradeoff for 10000 multiplications as a function of $\mu$. In this case, increasing $\mu$ with other parameters fixed increases the security of the system(confirms (1)) at the cost of decreasing the depth. With small values of $\mu$, the cost remains stable. However, for larger values, the cost increases considerably and this is due to the fact that, the coefficients of the random polynomial in \texttt{Encrypt} increase  which is reflected in the size of the ciphertexts. Hence it is profitable to increase $\mu$ to increase the security of the system when $\mu$ is not too large.

\begin{table}[htb]
  \centering
  \caption{Trade-off for 10000 multiplications}
\begin{tabular}{|c|c|c|c||}
  \hline
  \multicolumn{4}{|c||}{$\mu = 4$ , $N=8$} \\
  \hline
  $\eta$ &  time(in s)  & depth & $2^{\frac{N}{\epsilon}}$ \\
  \hline
  $2^{600}$ & 619  & 6.90 & 1.009 \\
  $2^{400}$ &  580 & 6.31 & 1.014 \\
  $2^{200}$ &  509 & 5.03 & 1.028 \\
  $2^{100}$ &  490 & 4.28 & 1.059 \\
   \hline
\end{tabular}
\normalsize
\label{tab:tradeofftab1}
\end{table}

\begin{table}[htb]
  \centering
  \caption{Trade-off for 10000 multiplications}
\begin{tabular}{|c|c|c|c||}
  \hline
  \multicolumn{4}{|c||}{$\eta = 2^{300}$ , $N=8$} \\
  \hline
  $\mu$ &  time(in s)  & depth & $2^{\frac{N}{\epsilon}}$ \\
  \hline
  $2$ & 527  & 6.90 & 1.01887 \\
  $4$ & 526 & 5.89 & 1.01894 \\
  $8$ & 528  & 5.63 & 1.01900 \\
  $16$ & 529  & 5.40 & 1.01907 \\
  $32$ & 801  & 5.21 & 1.01913 \\
  $64$ & 824  & 5.04 & 1.01920 \\

   \hline
\end{tabular}
\normalsize
\label{tab:tradeofftab2}
\end{table}


\section{Circuit Transformation}
As previously discussed, a user sends his program which runs on unencrypted data and his encrypted input to the cloud. The cloud then performs the demanded computation and returns the encrypted result. However, the program in the same form cannot always be used for computations on encrypted data. 

\textbf{Non-secret branching}: We consider a scenario where a user wants to compute the \texttt{xor} of an $n$ bit sequence $a_0,a_1,\ldots,$ \\ $a_{n-1}$. A program on the unencrypted bits would be :

 \phantom{x} \texttt{res=0, i=0;}
\newline \phantom{x} \texttt{do\{ }
\newline \phantom{x} \hspace{9ex} \texttt{res=res$\oplus$a\_i;}
\newline \phantom{x} \hspace{9ex} \texttt{i++;} 
\newline   \phantom{x} \} \texttt{while(i < n);} 
\newline  \phantom{x} \texttt{return res;}\\

This program can directly be used by the cloud, as it only has to check \texttt{if(i<n)} where $i$ and $n$ are unencrypted integers.

\subsection{Secret Branching}

Consider a program which involves the following test on two inputs : \texttt{if(a > b) instruction\_1; else instruction\_2;} If the cloud could know how to perform \texttt{if(a' > b') instruction\_1; else instruction\_2;} (where \texttt{a'} and \texttt{b'} are the encryptions of \texttt{a} and \texttt{b}) without executing both the instructions, he would know the decryption of the comparison \texttt{a'>b'}, which means that he would have information on \texttt{a} and \texttt{b}. The cloud transforms the actual program into : \texttt{test(a'> b')$\wedge$} \texttt{instruction\_1 +}$\neg$\texttt{test(a'> b')$\wedge$ } \\ \texttt{instruction\_2}, where \texttt{test()}  is a function on encrypted inputs \texttt{a'}, \texttt{b'} and returns an encryption of $1$ iff \texttt{a > b}. Hence, if the program used has branching i.e. it involves \texttt{if(cond\_0)} \texttt{\{inst\_0\} else if(cond\_1)\{inst\_1\}} \texttt{ ...else if(cond\_n-2) \{inst\_n-2\}} \texttt{else\{inst\_n-1 \}}; the cloud has to perform all the $n$ instructions i.e. the size of the circuit changes from a constant size to a size linear in the number of tests.

This increase in size is due to the fact that the cloud transforms a non-straight line program into a straight-line program.

\subsection{Secret Conditional Stopping}
We consider the following program that a cloud receives where \texttt{f} is a boolean function. : \\
 \phantom{x} \texttt{i=0, res=0;}
\newline \phantom{x} \texttt{do\{ }
\newline \phantom{x} \hspace{9ex} \texttt{i++;} 
\newline   \phantom{x} \} \texttt{while(i < n \&\& cond\_i);} 
\newline  \phantom{x} \texttt{res = f(i)};\\
In this case, the cloud needs to transform a loop with conditional stopping into a loop with unconditional branching. We propose the following transformation : \\
 \phantom{x} \texttt{cont=1', res=0', i=0;}
\newline \phantom{x} \texttt{do\{ }
\newline \phantom{x} \hspace{6ex} \texttt{res = res$\vee$(cont$\wedge$f(i)$\wedge\neg$cond\_i);}
\newline \phantom{x} \hspace{6ex} \texttt{cont = cont$\wedge$cond\_i;}
\newline \phantom{x} \hspace{6ex} \texttt{i++;} 
\newline   \phantom{x} \} \texttt{while(i < n);} 
\newline \phantom{x} \texttt{res=($\neg$(cond\_i)$\wedge$res)$\vee$(f(n)$\wedge$cond\_i);}
\newline 
\textbf{Correctness of the code :} We consider the following three cases :
 
1. \texttt{cond\_i} \textbf{is always} \texttt{true}: res equals $f(n)$ after the end of the loop.\newline
\phantom{x} 2. \texttt{cond\_i} is \texttt{true} for $i<t$ and \texttt{false} afterwards: \texttt{res} is \texttt{false} and \texttt{cont}=\texttt{true} for $i<t$. At $i=t$, \texttt{cond\_i}=\texttt{false}, so \texttt{res = f(t)}. For $i>t$ \texttt{cont}=\texttt{false} and so hence \texttt{res} retains its value till the end of the loop. \newline  
\phantom{x} 3. \texttt{cond\_i} is \texttt{false} for the first time at $i=t_1$ and becomes \texttt{true} at $i=t_2$ with $t_1<t_2$ : With the previous argument, \texttt{res=f(t\_1)}$\; \forall i<t_2$. At $i=t_2$, \texttt{$\neg$(cond\_i)}=\texttt{false} hence \texttt{res} retains the value \texttt{f(t\_1)} till the end of the loop.
 
We note that the cost of the above program is $n$ times the computational complexity of the function, however in the initial program the function is evaluated only once.

\subsection{ Didactic example :  Min-Max}
\label{sec:ex}
Suppose that a user wants to find the maximum and the minimum of two $n$-bit sequences $a = (a_0,a_1,\ldots, a_{n-1})$, $b=(b_0,b_1,\ldots, b_{n-1})$ where $a_0$ is the highest significant bit. He provides the following algorithm to the cloud :
\newline \phantom{x} \texttt{aIsGreater=0, i=0;}
\newline \phantom{x} \texttt{do\{ }
\newline \phantom{x} \hspace{6ex} \texttt{aIsGreater = a\_i$ \wedge \neg$(b\_i);} 
\newline \phantom{x} \hspace{7ex} \texttt{i++;} 
\newline   \phantom{x} \} \texttt{while(i<n \&\& (a\_i$\wedge$b\_i) $\vee$ ($\neg$(a\_i)$\wedge \neg$(b\_i));}
\newline \newline
where \texttt{ (a\_i$\wedge$b\_i) $\vee$ ($\neg$(a\_i)$\wedge \neg$(b\_i)} tests the equality of the bits.

We propose the transformed algorithm \autoref{Code:algo} of the abov
e program. It recasts the same ideas used for the previous example. The algorithm inspects the relative magnitude of pairs of bits, starting from the most significant bit and gradually proceeding towards lower significant bits until an inequality is found.
 
\newtheorem{theorem}{Lemma}
\begin{theorem}
Algorithm \autoref{Code:algo} returns the maximum and the minimum of the input bit sequences. 
\end{theorem}

\begin{proof}

  Consider $x_{-1}=1$ and $x_i = (a_i'\wedge b_i)'\vee (\neg(a_i')\wedge\neg(b_i')) $ in line 5. $x_i$ is $1$ iff $a_i'$ and $b_i'$ are equal. We denote by $bitEqual_{t}$ the value of $bitEqual$ at $i=t$ before assignment at line 5. In fact, $bitEqual_{t} = \bigwedge_{i<t}{x_i} $ at this stage and it is $1$ iff $a_i'=b_i'$ for all $i < t$. 

The term $\neg(b_i')\wedge a_i'$ at line 4 is $1$ iff $a_i'=1$ and $b_i'=0$. Hence at the iteration $i=t$, the algorithm using $bitEqual$ tests if $a'_i = b'_i \;\forall i<t$, while using $\neg(b_t')\wedge a_t'$, the algorithm tests if $a_t>b_t$. Hence $aIsGreater$ at $i=t$ is set to $1$ if all the bits till $i<t$ are equal and $a_t>b_t$. Once $aIsGreater$ is set to 1, it retains its value till the end of the loop.  

Max and Min regenerate $a_i$ and $b_i$ and hence contain the correct result.  
\end{proof}

\restylealgo{algoruled}

\linesnumbered

\begin{algorithm}[H]

\SetVline

 \KwData{$a'$:=$(a_0',a_1',\ldots, a_{n-1}'$), $b'$:=$(b_0',b_1',\ldots, b_{n-1}')$ }

 \KwResult{$Max(a',b')$ and $Min(a',b')$}

 $aIsGreater \leftarrow 0$\;
 $bitEqual \leftarrow 1$\;
	
 \For{$i\leftarrow 0 $ \KwTo $n-1$}{
						
     $aIsGreater \leftarrow aIsGreater \vee (\neg(b_i')\wedge a_i'\wedge bitEqual$) \;
     $bitEqual \leftarrow bitEqual\wedge((a_i'\wedge b_i')\vee(\neg(a_i)'\wedge\neg(b_i')))$		       	
 }

\For{$i\leftarrow 0$ \KwTo $n-1$}{

     $Max_i \leftarrow (aIsGreater \wedge a_i') \vee (b_i'\wedge \neg(aIsGreater))$ \;

     $Min_i \leftarrow  (a_i' \wedge \neg(aIsGreater) ) \vee (aIsGreater \wedge b_i')$ \;

 }

return($Max$, $Min$)

 \caption{Min-Max on cipher-text \label{Code:algo}}


\end{algorithm}


\textbf{Theoretical Cost :} Algorithm \autoref{Code:algo} uses $5n$ \texttt{AND} gates, $2n$ \texttt{OR} gates and $2n$ \texttt{NOT} gates to find the larger of the two bit sequences and then to regenerate the maximum and the minimum $4n$ \texttt{AND} gates and $2n$ \texttt{NOT} and \texttt{OR} gates. We note that reconstruction of Max and  Min is not required when working on non-encrypted data. Hence, this is an additional cost to pay to use the transformed program operating on encrypted data. 

\textbf{Experimental Analysis :} We measure the run time to perform min-max by varying $n=1$ to $32$ bits, \autoref{image_f2}. We find a linear cost which confirms the $\theta(n)$ complexity.

\begin{figure}[!h] %on ouvre l'environnement figure
\centering
\includegraphics[width=5cm,height=4cm]{f2.pdf} 
\caption{Run time(ms)} 
\label{image_f2} %l'étiquette pour faire référence à cette image
\end{figure}

\section{Choice of Benchmarks}
\label{sec:bm}

We propose a set of benchmark functions to evaluate the implementations. These functions can be categorized into three classes : functions operating on bits, functions operating on block of bits and functions where branching is usually required i.e \texttt{if-then-else} condition is evaluated.

Functions operating on bits provide information on the security-cost trade-off for bit-wise operations. The first two categories of functions have straight-line programs on non-encrypted data and hence can directly be translated to operate on encrypted bits. For others, evaluation of \texttt{if-then-else} condition on encrypted data is chosen. In this case, a circuit(program) transformation is required and which often makes the size longer than the one on non-encrypted data. Hence, this provides information on the increase in size of the circuit(when passing from non-encrypted to encrypted data) and the associated trade-off. We study the security-cost trade-off for the following problems : \newline
1. \textbf{XOR of an $n$-bit sequence}\\
2. \textbf{Majority bit of an $n$-bit sequence} \\
3. \textbf{Sum of :}  a) two unsigned integers of $32$ bits  \newline 
\phantom{x}\hspace{12ex} b) two unsigned integers of arbitrary  \newline \phantom{x}\hspace{15ex} number of bits  \newline
4. \textbf{Product of two $n\times n$ boolean matrices} \\
5. \textbf{Sorting of $n$ sequences of $nbit$ using :} \newline\noindent
\phantom{x}\hspace{3ex} a) Insertion sort \phantom{x}\hspace{2ex} b) Bitonic sort \\
	\phantom{x}\hspace{3ex} 	     c) Odd-Even Merge sort 

The interest of choosing the above sorting algorithms is that all of them can be parallelized and hence are ideal for distributed environments such as cloud. 

\subsection{Size of Evaluation Circuit}
As the implementation(\autoref{funs}) provides addition and multiplication of cipher-texts,  size in terms of \texttt{XOR} and \texttt{AND} gates are provided for each problem. \newline \newline
\textbf{XOR of $n$-bits :} \texttt{XOR} of $n$-bits requires $n-1$ \texttt{XOR} gates. \newline \newline
\textbf{Majority of $n$-bits :} Majority bit is evaluated by obtaining the sum of the bits and then returning the result of the comparison of the sum and $n/2$. The comparison is done using algorithm \autoref{Code:algo}. This operation is interesting in the sense that if the same program is evaluated on the user's side, he has to store the sum of the bits, hence  needs $\mathrm{\lfloor\log_{2}n \rfloor +1}$ bits of memory. However, if he performs the operation remotely on a cloud, he only requires a constant number of bits to retrieve the result. \newline \newline
\textbf{Sum of two unsigned integers of $32$ bits :} It is a sum of two $n$-bits sequences such as $n<32$ and it is performed using $n$ \texttt{full-adder} circuits (provided as \texttt{fhe\_fulladd(...)} function in the Scarab library). The cost of a full adder circuit is $2$ \texttt{XOR} to calculate the sum and $2$ \texttt{AND}, $1$ \texttt{XOR} and $1$ \texttt{OR} for the carry. Hence the total complexity is $3n$ \texttt{AND}, $5n$ \texttt{XOR}. \newline \newline
\textbf{Sum of two unsigned integers :} This program selects the first $32$-bits sequences of the $2$ integers and does the sum of them exactly like the previous algorithm and then perform the sum with next $32$-bits sequences with the carry from the previous addition until there is no sequence left. So this program has approximately the same linear cost as the previous one. \newline \newline
\textbf{Product of two $n\times n$ matrices :} Thes standard two loops algorithm requires $n^3$ \texttt{AND} gates and \texttt{$n^2(n-1)$} \texttt{XOR} gates. \newline \newline
\textbf{Sorting :} Several straight-line programs for sorting have have been proposed in literature \cite{dk}, \cite{Batcher:1968:SNA:1468075.1468121}. These programs use a comparator gate : a logical gate with two inputs and two outputs, that computes the minimum and maximum of two $n$ bit sequences. Algorithm \autoref{Code:algo} implements such a comparator gate. We do not need to perform any transformation for these algorithms. The algorithms used for Bitonic and Odd-Even merge sort are provided in the appendix. They accept $n$ which are powers of $2$.

\textbf{Insertion sort :} When allowing for parallel comparators, bubble sort and insertion sort are identical. Hence $n(n-1)/2$ comparator gates are used for insertion sort.

\textbf{Bitonic Sort :} The standard algorithm uses $n\mathrm{log}(n)·(\mathrm{log}(n)+1)/4$ comparator gates. 

\textbf{Odd-Even Merge sort :} Odd-Even merge sort requires  $n(\mathrm{log}^{2}(n)-1)/2 + 1 $ comparator gates.
We note that the sorting programs used do not increase the size of the circuit operating on encrypted input bit sequences. 

\section{Experimental evaluation \\ and results}
\label{Sec:Eval}
We execute the implementations of the above programs, and we measure their wall time and CPU time, and compare the experimental cost with the theoretical complexity. For each test, $5-10$ random samples were taken. These were generated using the Unix \texttt{rand()} function by setting the seed using \texttt{srand(time(NULL))}.  

 \autoref{fig:image_sf7} represents minimum and maximum wall time of \texttt{XOR}. We can see that the wall time increases linearly with number of bits which confirm the theoretical cost. We notice the same complexity for both \texttt{Bit Majority} \autoref{fig:image_sf0bis} and \texttt{sum of integers} \autoref{fig:image_sf1}. We further confirm the $O(n^{3})$ complexity for the \texttt{matrix product} \autoref{fig:image_sf2}. These experiments were performed on M2.


\begin{figure}[!H]%on ouvre l'environnement figure
\centering
\includegraphics[width=8cm, height=6cm]{f4.pdf} 
\caption{Wall time for XOR} 
\label{fig:image_sf0} %l'étiquette pour faire référence à cette image
\end{figure}

\begin{figure}%on ouvre l'environnement figure
\centering
\includegraphics[width=8cm, height=6cm]{f8.pdf} 
\caption{Wall time for bit majority} 
\label{fig:image_sf0bis} %l'étiquette pour faire référence à cette image
\end{figure}
\begin{figure}%on ouvre l'environnement figure
\centering
\includegraphics[width=8cm, height=6cm]{f8.pdf} 
\caption{Wall time for sum} 
\label{fig:image_sf1} %l'étiquette pour faire référence à cette image
\end{figure}

\begin{figure}%on ouvre l'environnement figure
\centering
\includegraphics[width=8cm, height=6cm]{f9.pdf} 
\caption{Wall time for matrix product} 
\label{fig:image_sf2} %l'étiquette pour faire référence à cette image
\end{figure}

 \textbf{Sorting :} 
The experiments on the sorting algorithms were performed on machine M1. 
Initially, the cryptographic parameters were set to $(N = 8, \nu = 2^{384}, \mu = 4, s_1 = 8, s_2 = 5)$ which is the default setting in the software. We measured CPU time and Wall time by varying the sequence length($nbit$) and the number of sequences(n). We plot maximum and minimum time in minutes for $nbit=6$ for insertion sort, \autoref{fig:image_sf4} . \autoref{fig:image_sf2} compares the results with changing values of $nbit$. \autoref{fig:image_sf1} presents a curve fitting plot that verifies the quadratic complexity of this algorithm.

\begin{figure} %on ouvre l'environnement figure
\centering
\includegraphics[width=10cm, height=10cm]{fsort4.pdf} 
\caption{Wall time for Insertion sort, nbit=6} 
\label{fig:image_sf4} %l'étiquette pour faire référence à cette image
\end{figure}


\begin{figure} %on ouvre l'environnement figure
\centering
\includegraphics[width=10cm, height=10cm]{fsort2.pdf} 
\caption{Wall time in minutes for Insertion sort} 
\label{fig:image_sf2} %l'étiquette pour faire référence à cette image
\end{figure}

Graphs for other values of $nbit$ and with wall time and CPU time are presented in Appendix. 

\begin{figure} %on ouvre l'environnement figure
\centering
\includegraphics[width=10cm, height=10cm]{fsort1.pdf} 
\caption{Curve fitting for Insertion sort, nbit=6} 
\label{fig:image_sf1} %l'étiquette pour faire référence à cette image
\end{figure}

\begin{table}
\caption{Wall time for Bitonic Sort}
\resizebox{8cm}{!}{
 
\begin{tabular}{|l|l|l||}
  \hline
  (Number of sequences,  & Maximum time (in ms) & Minimum time (in ms) \\
  Number of bits) &  &    \\
  \hline
  (4,4)  &  34584 & 32759\\
  (8,4)  &  189685 & 130157\\
  (16,4) &  635705 &  438146\\
  (32,4) &  1315663 & 1313082\\
  (4,6)  &  49668 & 49038 \\
  (8,6)  & 197853 & 196183 \\
  (16,6) & 667521 & 656369 \\
  (32,6) & 1976534 & 1946724 \\ 
 \hline
\end{tabular}
\label{bitonicsort}
}
\end{table}


\begin{table}
\caption{Wall time for Odd-Even Merge Sort}
\resizebox{8cm}{!}{
 
\begin{tabular}{|l|l|l||}
  \hline
  (Number of sequences,  & Maximum time (in ms) & Minimum time (in ms) \\
  Number of bits) &  &    \\
  \hline
  (4, 4) & 28363 & 27321  \\
  (8, 4)  & 158674 & 120868 \\
  (16,4) & 459895  & 340981  \\
  (32,4) & 1043093 &  1041674 \\
  (4,6) & 41350 & 41204 \\
  (8,6) & 158272  &156721 \\
  (16,6) & 543735 & 520719 \\
  (32,6) & 1638711 & 1594432 \\
\hline
\end{tabular}
\label{OddEvensort1}
}
\end{table}

\autoref{bitonicsort}, \autoref{OddEvensort1} justify the fact that Odd-Even Merge algorithm performs better than the Bitonic sort(even though they have the same worst case complexity) as it uses less number of comparator gates. We further remark that for small values, Insertion sort performs better than Bitonic, but we estimate that for $32$ sequences of $4$ bits each, Bitonic search would be faster. This estimate comes from the fact that the insertion sort would cost $16$ times the cost of sorting only $8$ such sequences i.e. around $3000000$ ms. This is experimentally confirmed by the obtained value : 3639429 which is thrice the time = 1315663 taken by Bitonic sort.    


\begin{table}
\caption{Wall time for Odd-Even Merge Sort with increased security}
\resizebox{8cm}{!}{
 
\begin{tabular}{|l|l|l||}
  \hline
  (Number of sequences,  & Maximum time (in ms) & Minimum time (in ms) \\
  Number of bits) &  &    \\
  \hline
  (4, 4) &  27958 & 27455\\
  (8, 4)  &    108197  & 103683\\
  (16,4) &   354188 &  341522\\
  (32,4) &  1587427 &  1044640\\
  (32,6) &   1629722 & 1610464\\
\hline
\end{tabular}
\label{OddEvensort2}
}
\end{table}

When we increase the security of the system by setting the parameters to  $(N = 256,\eta  = 16, \mu = 2, s_1 = 5000, s_2 = 5)$(as suggested in \cite{cryptoeprint:2009:571}) which ensures $25$ bits of security, the Odd-Even sorting algorithm on  $4$ sequences, each of $4$ bits takes hours. By lowering the security level to only $16$ bits using the parameters :$(N = 16, \eta = 32, \mu = 2, s_1 = 85, s_2 = 5)$, the result did not decrypt to the desired output; which confirms to the theory that $\epsilon$ must be greater than 1 for correct decryption. Lowering the security even further to only $2$ bits using the parameters: $(N = 8, \eta = 2^8, \mu = 4, s_1 = 65, s_2 = 5)$, and testing odd-even sorting for $32$ sequences of $6$ bits each took more than an hour. However, increasing $\mu$ also increases security, and setting the parameters to :  $(N = 8, \eta = 2^{384}, \mu = 8, s_1 = 8, s_2 = 5)$ we obtained the results as in \autoref{OddEvensort2}. The results show that this set of parameters provide a better security(than the default parameters) and at almost no extra cost. For this result to be conclusive, we need to perform tests for large $n$. For our last test, we set the parameters to  $(N = 8, \eta = 2^{200}, \mu = 8, s_1 = 8, s_2 = 5)$ which gave us incorrect decryption. We note that decryption fails in case the depth is too small for re-crypt to function.

\section{Conclusion}
With the examples and the programs discussed, we conclude that the transformation of a non-straight-line program on unencrypted data(with branching) into a straight line program on encrypted data increases the circuit complexity. We experimentally measured the cost of different operations on encrypted data and the security-cost trade-off for certain operations. The existing implementation of Smart-Vercauteren scheme remains practical only for very low level of security, when a user wants to apply a sorting algorithm on encrypted bit sequences of short length.

As the sorting algorithms used can be parallelized to obtain better results, it would be interesting to perform computations on GPU.        

\bibliographystyle{alpha}
\bibliography{article}  
\balancecolumns
% That's all folks!
\end{document}

